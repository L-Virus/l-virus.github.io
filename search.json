[{"title":"BERT","path":"/2025/10/26/BERT/","content":"模型 模型具有两个阶段，分别为预训练和微调 在预训练阶段进行无监督学习，输入是没有标签的数据 在微调阶段，使用预训练阶段得到的参数初始化模型，使用下游任务的标注数据进行训练，每个下游任务都有一个单独的微调模型 \\(L\\): Transformer Block \\(H\\): Hidden Size \\(A\\): Self-attention Head 文中使用两种模型： \\[\\displaylines{BERT_{BASE}(L=12,H=768,A=12,110M) \\\\ BERT_{LARGE}(L=24,H=1024,A=16,340M)}\\] Input 与 Output “our input representation is able to unambiguously represent both a single sentence and a pair of sentences (e.g., 〈 Question, Answer 〉) in one token sequence. Throughout this work, a “sentence” can be an arbitrary span of contiguous text, rather than an actual linguistic sentence. A “sequence” refers to the input token sequence to BERT, which may be a single sentence or two sentences packed together.” (Devlin 等, 2019, p. 4) (pdf) 首先论文中的sentence不仅仅只能是语言学上的完整句子，还可以是任意一段连续的文本片段。sequence，可以是刚刚讲到的sentence，以一个单独出现或者成对出现的形似使用WordPiece（一种Tokenize方法） “First, we separate them with a special token ([SEP]). Second, we add a learned embedding to every token indicating whether it belongs to sentence A or sentence B.” (Devlin 等, 2019, p. 4) (pdf) 每个sentence的开头都是[CLS]（一个可学习的向量，是后面token的最高级抽象，也就是代表最后的输出y） 句子对的进入则也是被打包成一个sequence，使用[SEP]来分割句子 使用 Segment Embedding 来告诉模型，token属于哪个sentence 而使用 Position Embedding 来告诉模型，token 在句子中的位置 所以 Embedding = Tokenize Embedding + Segment Embedding + Position Embedding（均是可学习的参数） 在 Pre-training 中的输入是无标签的单个或者一对的 sentence 在 Fine-tunning 中的输入是带标签的单个或者一对的 sentence Pre-training 在 Pre-training 的环节里有两个预测任务：Masked LM，Next Sentence Predict \\[ L_{total}=L_{MLM}+L_{NSP} \\] Data “For the pre-training corpus we use the BooksCorpus (800M words) (Zhu et al., 2015) and English Wikipedia (2,500M words).” (Devlin 等, 2019, p. 5) (pdf) 训练集为 BookssCorpus，English Wikipedia “It is critical to use a document-level corpus rather than a shuffled sentence-level corpus” (Devlin 等, 2019, p. 5) (pdf) 最好使用文档级别语料库而不是打乱了的句子级别的语料库 Masked LM（MLM） “In order to train a deep bidirectional representation, we simply mask some percentage of the input tokens at random, and then predict those masked tokens.” (Devlin 等, 2019, p. 4) (pdf) 为了使得模型能够训练得更深，需要训练一个Mask LM（MLM，也被称为完形填空任务Cloze task），作用是随机屏蔽一定比例（每个序列中屏蔽 15% 的token）的 token，然后再预测这些被屏蔽的 token（就是隐藏状态进入Linear和Softmax），学习句内关系 目标函数 \\[ L_{MLM}=-\\sum_{masked tokens}=logP(原始词语|上下文) \\] 分布不一致及解决方案 “Although this allows us to obtain a bidirectional pre-trained model, a downside is that we are creating a mismatch between pre-training and fine-tuning, since the [MASK] token does not appear during fine-tuning. To mitigate this, we do not always replace “masked” words with the actual [MASK] token. The training data generator chooses 15% of the token positions at random for prediction. If the i-th token is chosen, we replace the i-th token with (1) the [MASK] token 80% of the time (2) a random token 10% of the time (3) the unchanged i-th token 10% of the time. Then, Ti will be used to predict the original token with cross entropy loss.” (Devlin 等, 2019, p. 4) (pdf) 由于在 fine-tunning 中并不会出现像 pre-training 一样出现[MASK]，所以导致训练和推理分布的不一致性，因此在实际训练中，[[#^d36887|每个序列中屏蔽15%的token]] 中，80% 使用[MASK]，10% 使用随机的token（增加鲁棒性），10% 保持原 token 不变。最后模型使用交叉熵损失来预测原来的 token Next Sentence Predict（NSP） “In order to train a model that understands sentence relationships, we pre-train for a binarized next sentence prediction task that can be trivially generated from any monolingual corpus.” (Devlin 等, 2019, p. 4) (pdf) 因为例如 Question and Answering(QA)，Natural Language Inference(NLI)，是基于对两个句子之间关系的理解而构建的，而预训练语言模型是做不到这点的，因此预训练一个二值化的NSP任务来捕捉句间关系；在实验设置中，50%是下一句，50%为随机抽取的 目标函数 \\[ L_{NSP}=-[ylogP(IsNext)=(1-y)logP(NotNext)] \\] Fine-tunning 文本对的处理 之前的方法 “a common pattern is to independently encode text pairs before applying bidirectional cross attention” (Devlin 等, 2019, p. 5) (pdf) 分别将编码两个文本（sentence A B），得到向量表示 再通过 bidirectional cross attention（互相关注对方信息）来建模交互。 文本的方法 “uses the self-attention mechanism to unify these two stages” (Devlin 等, 2019, p. 5) (pdf) 将两句话 拼接 作为单个序列输入 [CLS] A [SEP] B [SEP]。 使用 self-attention 直接在整个序列上计算注意力。 结果： [A tokens] 可以关注 [B tokens] [B tokens] 也可以关注 [A tokens] 相当于 一步实现了双向 cross attention，不需要分两步。 Tunning “For each task, we simply plug in the taskspecific inputs and outputs into BERT and finetune all the parameters end-to-end.” (Devlin 等, 2019, p. 5) (pdf) 由于在 Pre-training 中利用 Masked LM（MLM）学到了句内关系，在 Next Sentence Predict（NSP）中学会了句间关系，使得参数在进行 fine-tunning 的时候能够更加灵活的学习具体的任务模式 Task Input Paraphrasing（语义相似性/复述检测） Entailment（文本蕴含/推理） Question Answering（问答） Text classification / sequence tagging（文本分类/序列标注） Output Token-level NER（Named Entity Recognition，命名实体识别） SQuAD（Stanford Question Answering Dataset，问答） Sequence-level MNLI（Multi-Genre Natural Language Inference，多领域自然语言推理） 前言 “There are two existing strategies for applying pre-trained language representations to downstream tasks: feature-based and fine-tuning” (Devlin 等, 2019, p. 1) (pdf) 现如今预训练语言表示主要有两种方法：基于特征，微调，使用单向语言模型来学习。 两种预训练语言表示方法基于特征的方法主要是：当一个预训练好的语言表示放进下游任务进行训练时，词向量作为特征，不会随着训练而变化，主要代表是 ELMo 微调的方法主要是：当一个预训练好的语言表示放进下游任务进行训练时，词向量作为特征，会随着训练而进行变化，主要代表是 GPT BERT 是结合基于特征和微调两种方法的预训练语言模型，前者是在 pre-training 中，后者是在 fine-tunning 中 核心 “BERT alleviates the previously mentioned unidirectionality constraint by using a “masked language model” (MLM) pre-training objective” (Devlin 等, 2019, p. 1) (pdf) 双向的、基于Transformer编码器(Encoder)的、使用掩码的语言模型 模型将输入的词随机掩盖，希望模型能仅从上下文中预测到被掩盖的词，这样就融合了上下文的特征来学习 改进 首次提出双向语言表示模型 使用掩码语言模型来使用上下文信息 而不是简单训练两个分别从左到右和从右到左的模型的浅层串联，GPT-1 都还是单向的 使用下一句话预测的任务来训练文本对表示 迁移学习 受到在大数据集上进行迁移学习的有效性和计算机视觉中 pre-trained model 和 fine-tune 的启发 才使得 BERT 有了两个阶段：pre-training 和 fine-tunning 训练 batch size = 256 epoch = 40 Adam（lr = 1e-4，\\(\\beta_1=0.9,\\beta_2=0.999\\)) L2 weight decay =0.01 dropout = 0.1 (all) activate = gule loss = \\(\\frac{mean \\log mlm+mean \\log nsp}{2}\\) 微调 batch size = 32 epoch = 3 GLUE Test 任务 类型 输入 输出 描述 QQP Sequence-level 问题对（question1, question2） 是否语义重复（Yes/No） Quora Question Pairs QNLI Sequence-level 句子对（question, sentence） 是否蕴含（entailment/none） Question Natural Language Inference SST-2 Sequence-level 单句 情感分类（positive/negative） Stanford Sentiment Treebank CoLA Sequence-level 单句 语法正确/错误（acceptability） Corpus of Linguistic Acceptability STS-B Regression 句子对 相似度分数（0–5） Semantic Textual Similarity Benchmark MRPC Sequence-level 句子对 是否语义等价（Yes/No） Microsoft Research Paraphrase Corpus RTE Sequence-level 句子对（premise, hypothesis） 是否蕴含（entailment/none） Recognizing Textual Entailment 变体 RoBERTa A Robustly Optimized BERT 改进 方面 BERT RoBERTa 改进效果 训练数据 ~16GB 160GB+（BookCorpus + CC-News + OpenWebText + Stories） ×10 数据量 动态 Mask 静态 mask 一次 每次 epoch 随机 mask 防止 overfit 掩码 训练时长 1M steps 更久 + 更大 batch (≈8K) 学得更扎实 去掉 NSP 有 Next Sentence Prediction ❌ NSP 任务直接移除 证明 NSP 没用 学习率 warmup 较保守 大 LR + 大 warmup + longer training 更鲁莽但有效 batch size 256 / 512 8k / 32k 大幅提升训练稳定性 动态Mask 每次训练迭代重新随机 Mask 句子中的词，而不是像 BERT 那样只 Mask 一次后固定不变。 DeBERTa Decoding-enhanced BERT with Disentangled Attention 改进 1. 解耦及相对位置编码 Disentangled Attention 解耦 “词的内容” “词的位置” Relative Position 相对位置编码 BERT 把 word embedding 和 position embedding 直接加起来 → 混在一起，使得模型无法明确区分“是什么词”和“在什么位置” 而DeBERTa 不这样，它是： 每个 token 变成两个可学习的 embedding： 内容 embedding（content） → 这个词是什么 位置 embedding（position） → 它跟别的词相对几位 \\[ Embedding=content \\ embedding + position \\ embedding \\] 然后注意力计算变成 跨类型 attention，显式的学习位置对值的影响： \\[ \\begin{align} QK^T = (Q^c+Q^p)(K^c+K^p)^T \\\\ =Q^c {(K^c)}^T + Q^c {(K^p)}^T + Q^p {(K^c)^T} + Q^p {(K^p)}^T \\end{align} \\] 其中根据文献啥忘记了，最后一位 \\(Q^p {(K^p)}^T\\) 由于是相对位置编码故没有过多有效信息所以省略。因此， \\[ QK^T = Q^c {(K^c)}^T + Q^c {(K^p)}^T + Q^p {(K^c)^T} \\] 2. 不变微调 scale-invariant-Fine-Tunning（SiFT） 在BERT中提过，在遇到小数据集的效果不好，因此 DeBERTa 采用 SiFT 来提高泛化性 1. 计算扰动 先计算普通损失的梯度，在进行归一化（保证方向正确）并放大 \\[ \\displaylines{ g= abla_{\\theta}L(\\theta)\\\\ r_{adv}=\\epsilon \\frac{g}{\\parallel g \\parallel_2} } \\] 其中 \\(\\epsilon\\) 为超参数，\\(r_{adv}\\) 为扰动 2. 扰动训练 \\[ \\displaylines{ L_{adv}=l(\\theta+r_{adv}) \\\\ \\theta \\leftarrow \\theta - \\eta abla_{\\theta} L_{adv} } \\]","tags":["NLP","BERT","WordEmbedding"]},{"title":"Word Embedding","path":"/2025/10/26/Word Embedding/","content":"前言 为了探究主流 Word Embedding 技术的变化，故写下这边文章。主要参考车万翔老师的《自然语言处理 基于预训练模型的方法》 Word Embedding 分为两个时期，一个是 Word2Vec 出来之前的远古时期，一个是 Word2Vec 出现之后的现代时期 One-hot Encoding 假设词表 \\(\\mathcal{V}\\)，则大小为 \\(\\mid \\mathcal{V} \\mid\\) ，则词表中第 \\(i\\) 个词 \\(W_i\\) 表示为向量则为， \\[ e_{w_i}=[0,0,\\dots,1,\\dots,0] \\in \\{0,1\\}^{\\mid \\mathcal{V} \\mid} \\] 在该向量中，第 \\(i\\) 个词在第 \\(i\\) 维上被设置成1，其余均为0。 优点 针对变量少，变量相关性不大的数据来说很方便 问题 相似度 尽管两个词在语义上很相近，但是在经过 One-hot Encoding 后，两个向量得到的相似度为0 冗余 这种 Encoding 模型会导致参数矩阵变得稀疏 Sparsity 且冗余 缓解办法 引入 WordNet 等语义词典，然后引入它们的共同语义信息作为新的额外特征，从而缓解同义词的独热表示不同的问题。 分布式语义假设 人们在阅读过程中遇到从未见过的词时，通常会根据上下文来推断其含义以及相关属性。 有点类似skip-gram的思想，自监督 分布式语义假设：词的含义可由其上下文的分布进行表示 基于这种假设，可以利用大规模的未标注文本数据，根据每个词的上下文分布对词进行表示。 词语共现频次表 假设 corpoc 中有三句话： 我 喜欢 自然 语言 处理 。 我 爱 深度 学习 。 我 喜欢 机器 学习 。 假设以词所在句子中的其他词语作为上下文。构建词语共现频次表 表中的每一项代表一个词 \\(wi\\) 与另一个词 \\(w_j\\)（上下文）在同一个句子中的共现频次，每个词与自身的共现频次设置为0。 我 喜欢 自然 语言 处理 爱 深度 学习 机器 。 我 0 2 1 1 1 1 1 2 1 3 喜欢 2 0 1 1 1 0 0 1 1 2 自然 1 1 0 1 1 0 0 0 0 1 语言 1 1 1 0 1 0 0 0 0 1 处理 1 1 1 1 0 0 0 0 0 1 爱 1 0 0 0 0 1 1 1 0 1 深度 1 0 0 0 0 0 0 1 0 1 学习 2 1 0 0 0 1 1 0 1 1 机器 1 1 0 0 0 0 0 1 0 1 。 3 2 1 1 1 1 1 2 1 0 优势 不再如同 One-hot Encoding 那样每个词向量之间不包含相似度 扩展 上下文的选择有很多种方式，而选择不同的上下文得到的词向量表示性质会有所不同。例如，可以使用词在句子中的一个固定窗口内的词作为其上下文，跟 CBOW 的思想很像。 问题 高频词误导计算结果 共现频次无法反映词之间的高阶关系 仍然存在稀疏性的问题 当矩阵过大时，奇异值分解速度慢 当在原来的语料库基础上新增数据，则需要重新计算奇异值分解 只能表达短的单元，如词、短语，不能表达长的，例如句子、段落 缓解办法 解决高频词误导计算结果 核心思想：如果一个词与很多词共现，则降低其权重；反之，如果一个词只与个别词共现，则提高其权重。 使用点互信息（Pointwise Mutual Information，PMI），对于词 \\(w\\) 和 上下文 \\(c\\)，有 \\[ PMI(w,c)=log_2\\frac{P(w,c)}{P(w)P(c)} \\] 通过这个计算，如果 \\(w\\) 和 \\(c\\) 的共现概率（与频次正相关）较高，但是 \\(w\\) 或者 \\(c\\) 出现的概率也较高（高频词），则最终的\\(PMI\\)值会变小；反之，即便 \\(w\\) 和 \\(c\\) 的共现概率不高，但是 \\(w\\) 或者 \\(c\\) 出现的概率较低（低频词），则最终的\\(PMI\\)值也可能会比较大。从而较好地解决高频词误导计算结果的问题。 Note由于 \\(log\\) 的分布问题，会导致一些低频率词的大小为负数，使得统计方差变大，因此在实际应用中通常采用PPMI （Positive PMI） \\[PPMI(w,c)=max(PMI(w,c),0)\\] 此外还可以使用，信息检索中常用的TF-IDF 共现频次无法反映词之间的高阶关系 以奇异值分解为例，对词语共现矩阵 \\(M\\) 做奇异值分解， \\[ M =U\\Sigma V^{T} \\] 词嵌入表示 为了解决分布式语义假设中的诸多问题，引入这种新的词表示方法 与分布式表示不同的是，词向量中的向量值，是随着目标任务的优化过程自动调整的，也就是说，可以将词向量中的向量值看作模型的参数 词袋模型 词袋模型（Bag of Word BOW）,核心思想就是假设文本中的词语是没有顺序的集合，将文本中的词按照向量进行表示，也就是将整个句子或文档表示为一个词频向量（统计每个词出现次数）。 与 One-hot Encoding 不同的是，前者是一整个句子或者文档就是一个词频向量，而后者是一个词为一个向量。 与分布式语义假设不同的是，前者不考虑词语在句子中的顺序。 例子 词表：[“我”，“爱”，“自然”，“语言”，“处理”] 句子：“我 爱 自然 语言 处理 爱 自然” 对应的BOW向量则为：\\([1, 2, 2, 1, 1]\\) 问题 没考虑到词的顺序信息 无法融入上下文信息，虽然能增加二元词的词向量，但是随着词表的越大，数据稀疏越严重 Word2Vec 预测任务驱动的神经网络模型 输入一般是 One-hot Encoding CBOW 连续词袋模型，基本思想是根据上下文对目标词进行预测 CBOW模型的任务是根据一定窗口大小内的上下文 \\(C_t\\)（若取窗口大小为5，则(\\(C_t=w_{t−2},w_{t−1},w_{t+1},w_{t+2}\\)）对 \\(t\\) 时刻的词 \\(w_t\\) 进行预测，即 \\(P(w_t|C_t)\\) 实际上就是，隐含层执行对词向量层取平均的操作，而没有线性变换以及非线性激活的过程。 Skip-gram 在 CBOW 的基础上继续做出简化，使用 \\(C_t\\) 中的每个词作为独立的上下文对目标词进行预测。 总计 以上模型可以归纳为对目标词的条件预测任务，CBOW 为根据上下文预测当前词，Skip-gram 则是根据当前词预测上下文。 负采样方法提供了一种新的任务视角：给定当前词与其上下文，最大化两者共现的概率。 这样问题就被简化为对于（\\(w,c\\)）的二元分类问题（共现或者非共现），规避了大词表上的归一化计算 GloVe 结合词向量以及矩阵分解的思想 基本思想是利用词向量对“词–上下文”共现矩阵进行预测（或者回归），从而实现隐式的矩阵分解 首先根据窗口大小构建窗口词语频次共现矩阵，与词语共现频次表不同的是，这边要根据窗口大小，后者的窗口大小可以认为是整个句子的长度 其中矩阵中每个元素为\\(X_{ij}\\)=词 \\(j\\) 在 词 \\(i\\) 的上下文窗口中出现的次数 将频次转换为概率 \\(P_{ij}=\\frac{X_{ij}}{\\Sigma_kX_{ik}}\\) ，表示词\\(i\\)在上下文看到词\\(j\\)的概率 进行学习词向量，使得 \\(w_i^T \\overset{\\sim}{w_j}+b_i+\\overset{\\sim}{b_j}\\approx log(X_{ij})\\) Why使用 \\(log\\) 的含义是对原式子进行缩放，使得，放缩动态范围（减少高频词主导），保持比例关系（共现越多，值越大） ELMo 不管是 Word2Vec 还是 GloVe，都是属于静态词向量，即对于任何一个词，其向量表示是恒定的，不随上下文的变化而变化。 因此为了刻画一个词在不同上下文或不同语境下的不同词义信息，引入动态构建词向量的方法，也称上下文相关的词向量 输入层 为字符级，使用1d CNN，能捕捉词形信息 双向LSTM 同时训练两个模型，正向语言模型预测下一个词 ：\\(P(w_t|w_{1},\\dots,w_{t-1})\\)，反向语言模型预测上一个词：\\(P(W_t|w_{t+1},\\dots,w_T)\\)， 输出层 把每层隐藏层的两个模型的隐状态拼接起来（\\(h_{t,j}=[\\overset{\\leftarrow}{h_{t,j}};\\overset{\\rightarrow}{h_{t,j}}]\\)），再加权组合\\[ELMo_t=\\gamma\\sum{^{L}_{j=0}}s_{j}h_{t,j}\\]其中 \\(\\gamma\\) 为可学习的放缩参数，\\(s_j\\) 为可学习的权重（经过Softmax后），\\(h_{t,j}\\) 为第 \\(j\\) 层的隐藏状 Note整个模型的框架就是RNN BERT 根据 BERT 中的描述，主要是利用一个双向的，纯 Transformer 的架构，在 pre-training 中利用 MLM 和 NSP 两个子任务来学习语言表示，再在 Fine-tunning 阶段动态的微调这些语言表示。","tags":["NLP","WordEmbedding"]},{"title":"U-Net：Convolutional Networks for Biomedical  Image Segmentation","path":"/2025/10/26/U-Net：Convolutional Networks for Biomedical  Image Segmentation/","content":"前言 “In this paper, we build upon a more elegant architecture, the so-called “fully convolutional network”” (Ronneberger 等, 2015, p. 2) (pdf) 模型是建立在全卷积网络上 模型 一个U型的结构，左边是下采样进行提取特征，右边是上采样进行恢复特征图，中间是瓶颈层将大量局部特征整合成更高层抽象信息，减少冗余，提供非线性变换，为解码阶段提供新的特征表示，模型在每层下降的过程中会保存下降的结果与对应上升的特征图进行跳跃连接。 输入为 \\(572 \\times 572\\) ，进行下采样每层经过 \\(3 \\times 3\\) 无填充的卷积、ReLU和一个大小为 \\(2 \\times 2\\)，步长为2 的maxpooling，同时增加一倍通道数 因为在上采样过程中图像大小与在下采样中不同，使用需要裁剪特征图来跳跃连接 Note\\(1 \\times 572 \\times 572\\) 经过两次卷积和两次ReLU后，大小为 \\(64 \\times 568 \\times 568\\) (572-3+1-3+1) ； 经过maxpooling 后减少特征图大小，大小为 \\(64 \\times 284 \\times 284\\) ((568-2+2)/2)； 直到最底层大小为 \\(512 \\times 32 \\times 32\\) 达到瓶颈层后经历两次卷积块后大小为 \\(1024 \\times 28 \\times 28\\) 在上采样阶段使用反卷积和下降的特征图做拼接，并且通道数量减半到大小为 \\(64 \\times 388 \\times 388\\) 最后进行 \\(1 \\times 1\\) 卷积，将通道数降到图像中类别数 在后续的U-net优化中，应该使上下采样的大小相同 模型优化 1.重叠切片（无缝平铺）+镜像填充 因为卷积网络在经过下采样和上采样后，图片中的物体边缘精度会降低，因为在U-net中使用一种名为重叠切片+镜像填充的方法解决 2.弹性形变 使用弹性形变来进行充分的数据增强，因为在医学图像中，组织的形变是最常见的变化，Dosovitskiy [Dosovitskiy, A., Springenberg, J.T., Riedmiller, M., Brox, T.: Discriminative unsupervised feature learning with convolutional neural networks. In: NIPS (2014)] 在无监督特征学习的范围内已经证明了数据增强对学习不变性的价值。 3.加权损失 为了解决细胞分割中的一个挑战：分离同一类的接触物体； U-net使用加权损失的方法：接触细胞之间的分离背景标签在损失函数中获得较大的权重。 这样能更好的分离出两个接触细胞之间的背景信息 在后续的优化中，我们更通常使用Dice loss 或 Focal loss，Boundary loss / Hausdorff loss 4.选择合适的池化窗口大小 为了保证 tile 能被网络多层池化后尺寸仍然对齐，需要选择合适的 tile 大小（通常是 \\(2^N\\) 的倍数 + U-Net 卷积裁剪调整）。 数据 ISBI 2015 训练 优化器：SGD batch_size：1 momentum：0.99 loss：softmax、cross-entropy init_weight：使用标准差为 \\(\\sqrt{2/N}\\) 的高斯分布，其中N为每层输入的像素点数 数据增强 “We generate smooth deformations using random displacement vectors on a coarse 3 by 3 grid. The displacements are sampled from a Gaussian distribution with 10 pixels standard deviation. Per-pixel displacements are then computed using bicubic interpolation. Drop-out layers at the end of the contracting path perform further implicit data augmentation.” (Ronneberger 等, 2015, p. 6) (pdf) 在粗糙的 3 x 3 网格上使用随机位移矢量生成平滑变形。位移是从具有 10 像素标准差的高斯分布中采样的。然后使用双三次插值计算每像素位移。收缩路径末端的辍学层执行进一步的隐式数据扩充。 以及正常的旋转和变色","tags":["U-net，CV"]},{"title":"Conda的使用","path":"/2025/10/26/Conda的使用/","content":"一、Conda的管理 1.1 检查Conda版本 conda --version 1.2 列出虚拟环境 conda env list 二、管理虚拟环境 2.1 创建环境 conda create -n env-name python==3.x 2.2 激活环境 基础环境为base windows：conda activate env-name linux： source activate env-name 2.3 退出环境 windows：conda deactivate env-name linux： source deactivate env-name 2.4 删除环境 conda env remove -n env-name","tags":["Conda"]},{"title":"在Stellar中设置Pandoc","path":"/2025/10/15/在Stellar中设置Pandoc/","content":"前言 在Stellar中默认的公式渲染器功能不太强大，折腾了两小时发现使用Hexo-renderer-pandoc能够很好的解决默认公式渲染器的问题 下载pandoc 首先先到Pandoc的github官方仓库中下载最新的msi（如果你的Windows） 按照教程安装好，记住安装位置 下载hexo-renderer-pandoc 也是一样进入hexo-renderer-pandoc的仓库中 其实进不进都无所谓的，仓库里的README也可以自己参考 1、删除旧配置 npm un hexo-renderer-marked 2、下载hexo-renderer-pandoc npm install hexo-renderer-pandoc _config中的设置 又到大家喜欢的抄配置环节了 # Pandocpandoc: pandocPath: D:/Pandoc/pandoc.exe args: - --mathjax # 启用 MathJax 渲染公式 # - --filter=pandoc-crossref # 这个要求再装一个包来使用交叉引用 - --wrap=preserve # 防止代码换行破坏公式 # - --standalone # 会让整个页面变形 - --metadata - link-citations=true extensions: - +tex_math_dollars # 允许 $...$ 行内公式 - +raw_tex # 保留原始 LaTeX 代码# Markdownmarkdown: render:pandoc","tags":["Hexo","瞎折腾","Stellar"]},{"title":"Attention Is All You Need","path":"/2025/10/15/Attention Is All You Need/","content":"模型 Input与Output 个人认为Input与Output之间画一条指向Output的线，毕竟，Output Embedding的输入只是Input Embedding输出的下一个输入，图中提到的shifted right 类似与滑动窗口往右边移动 Positional Encoding 使用不同频率的 \\(sin\\) 和 \\(cos\\) 函数来作为位置编码，使用两个函数来代表奇数项和偶数项的位置，唯一的超参数为 \\(d_{model}\\) 特征的大小 \\[\\displaylines{PE_{(pos,2i)}=sin(pos/10000^{2i/d_{model}}) \\\\ PE_{pos,2i+1}=cos(pos/10000^{2i/d_{model}})}\\] 这样得到一个与输入embedding大小一样的位置编码矩阵，直接加在embedding中 我们对输入进行分解\\(QK^T=(E+P)(E+P)^T=EE^T+EP^T+PE^T+PP^T\\) \\(EE^T\\)：表示语义间关系 \\(EP^T,PE^T,PP^T\\)：表示引入位置信息的影响 Scaled Dot-Product Attention Step1 首先关于如何计算出 \\(Q、K、V\\) 矩阵 将输入的token进行embedding变换成计算机输入的向量格式 生成三个可学习的参数矩阵 \\(W_q、W_k、W_v\\) 假设有4个输入为 \\(a_i,i \\in [1,2,3,4]\\) ，也就是 \\(X\\) \\(Q\\)矩阵为，\\(W_q \\times a_i,i \\in [1,2,3,4]\\)，也就是 \\(Q=XW_q\\) \\(K\\)矩阵为，\\(W_k \\times a_i,i \\in [1,2,3,4]\\)，也就是 \\(K=XW_k\\) \\(V\\)矩阵为，\\(W_v \\times a_i,i \\in [1,2,3,4]\\)，也就是 \\(V=XW_v\\) Step 2 计算注意力 \\[Attention(Q,K,V)=Softmax(\\frac{QK^{T}}{\\sqrt(d_k)})V\\] 其中 \\(Q\\) 为 \\(R^q\\) 上大小为 \\(n \\times d_k\\) ，\\(K\\) 为 \\(R^k\\) 上大小为 \\(m \\times d_k\\) , \\(d_k\\) 为自定义超参，\\(V\\) 为 \\(R^v\\) 上大小为 \\(m \\times d_v\\) ，最终的大小为 \\(n \\times m\\) Softmax是对最后一个维度进行操作，也就是词汇表维度 Cross-Attention 使用编码器的输出与对应 \\(W_k、W_v\\) 矩阵进行相乘后计算出 \\(K和V\\)，而解码器中的输入作为 \\(Q\\)，进行计算。实际上，\\(Q\\) 为上一个输入之后的隐状态，K和V则表示为新的输入 \\[Q=XW_Q，K=YW_K,V=YW_V\\] Muti-Head Attention \\[\\displaylines{MutiHead(Q,K,V)=Concat(head_1,...head_h)W^o \\\\ Where \\ head_i=Attention(QW_i^q,KW_i^k,VW_i^v)}\\] “In this work we employ h = 8 parallel attention layers, or heads. For each of these we use dk = dv = dmodel/h = 64. Due to the reduced dimension of each head, the total computational cost is similar to that of single-head attention with full dimensionality.” (Vaswani 等, 2023, p. 5) (pdf) 针对head数来缩小d的维度（具体是使用W的大小来设置的），这样能在最后concat的时候保持维度一致 \\(W\\) 的作用：1. 缩小d的维度。2.学习更多不同的参数 并且使用多个头是为了学习不同的模式，所以使用不同的权重信息 FFN \\[FFN=max(0,xW_1+b_1)W_2+b_2\\] 其中，\\(max(0,xW_1+b_1)\\) 为第一次线性变化后使用ReLU进行非线性变化 “The dimensionality of input and output is dmodel = 512, and the inner-layer has dimensionality dff = 2048.” (Vaswani 等, 2023, p. 5) (pdf) \\(W_1\\) 将 \\(x\\) 的512维度扩大到2048，\\(W_2\\) 则又转会512 Tricks 1.Masked Attention “We also modify the self-attention sub-layer in the decoder stack to prevent positions from attending to subsequent positions.” (Vaswani 等, 2023, p. 3) (pdf) mask attention 因为在编码器中是使用自回归模型，模型会看到整个的输入，然而在预测也就是在解码器中，我们不需要看到全部输入，故使用掩码注意力来遮盖后面的部分 改进 1.针对RNN “This inherently sequential nature precludes parallelization within training examples,” (Vaswani 等, 2023, p. 2) (pdf) 这种固有的顺序性质排除了训练示例中的并行化 2.Norm （在三维中，对谁norm，就是把谁完整的切下来） 使用LayerNorm，而不是BathNorm Norm在Transformer中输入的sequence序列长度不一，如果进行batchnorm的话会导致统计量不稳定 优势 1.利用依赖关系进行建模 “allowing modeling of dependencies without regard to their distance in the input or output sequences” (Vaswani 等, 2023, p. 2) (pdf) “In this work we propose the Transformer, a model architecture eschewing recurrence and instead relying entirely on an attention mechanism to draw global dependencies between input and output.” (Vaswani 等, 2023, p. 2) (pdf) 依赖注意力机制来绘制输入和输出之间的全局依赖关系，对依赖关系进行建模，而不考虑它们在输入或输出序列中的距离。 2.进行并行计算 训练 优化器：Adam，\\(\\beta_1=0.9，\\beta_2=0.98，\\beta_3=0.98，\\epsilon=10^{-9}\\) 学习率：\\[l_{rate}=d_{model}^{-0.5}min(step_num^{-0.5},step_num \\times warmup_steps^{-1.5})，\\\\其中warmup_steps=4000\\] 大概意思是训练步骤的学习率线性增加，随后与步骤数的平方反比根成比例地减少学习率。 正则化： dropout（\\(P_{dropout}=0.1\\)）： 在每个子层中，残差连接之前使用dropout 在embedding和positional encoding相加后进行dropout label smoothing：设置 \\(\\epsilon_{ls}=0.1\\) ,虽然对perplexity不利但是有利于提升准确性和BLEU LabelSmoothing设类别数为：\\(K\\) 平滑系数为：\\(\\epsilon \\in [0,1]\\) 平滑后的目标分布为 \\[ y_i^{LS}= \\begin{cases} 1-\\epsilon,\\ 如果i=真实类别, \\\\ \\frac{\\epsilon}{K-1}， elses \\end{cases} \\]","tags":["Transformer","NLP"]},{"title":"使用Pycharm进行远程ssh（以Featurize为例）","path":"/2025/09/24/使用Pycharm进行进程ssh（以Featurize为例）/","content":"介绍应用背景 在使用 Pycharm 专业版的时候进行远程ssh连接服务器（Featurize）的 Python解释器和 Jupyter 远程连接Python 打开Pycharm点击右下角环境配置 点击添加新的解释器 选择基于SSH 选择新建 输入主机、端口以及用户名 以Featurize为例子，打开工作区右边的倒三角形 选择Pycharm连接 进入跟着指南操作即可 # 连接Jupyter 进入工作区打开Terminal 输入jupyter打开jupyter服务 在自己本机上打开终端（win+R），输入cmd，回车进入终端 进行端口映射 在终端输入 ssh -L8888:localhost:8888 featurize@workspace.featurize.cn -p port 上述操作是将Featurize服务器中的8888（Jupyter的服务端口）端口映射到本地，port为你的机子在Featurize中的端口，这些信息都可以在教程中查看。 按下回车，输入密码即可完成端口映射 进入Pycharm，点击右上角切换Jupyter服务器 选择配置Jupyter服务器 进入Featurize找到token 输入服务器地址和找到的令牌","tags":["Python","服务器"]},{"title":"安装PyTorch及Torchvision","path":"/2025/09/24/安装PyTorch及Torchvision/","content":"1.创建虚拟环境 具体查看我之前写的 《在Windows中利用Python的venv和virtualenv创建虚拟环境》 2. 检查显卡版本和CUDA 这种情况是需要电脑上有单独的英伟达的显卡、或者英伟达的显卡和集显这两种情况都是可以的。 3. 下载链接 进入PyTorch官网选择合适自己设备的下载链接 4. 下载 进入cmd 输入上一步获得的下载链接 5. 等待 6. 检测 下载完成后，进入python环境检测torch是否可用 7.下载轮文件 轮文件官网 按CTRL+F 搜索对应版本的文件 使用Motrix进行下载轮文件（只是因为torch太大采用下载器下载，其他可直接使用浏览器下载），版本对应如下 8.下载Torchvision github pytorch和torchvision版本对应官网 我的对应版本为","tags":["Python","Deep Learning"]},{"title":"Stellar设置专栏（topic）","path":"/2025/09/24/Stellar设置专栏/","content":"专栏文件夹的配置 在 blog/source/_data/ 文件夹中创建一个 topic 文件夹，在其中放入各个专栏的描述文件，文件名就是项目的 id： name: Stellar # 在面包屑导航上会显示较短的名字title: Stellar - 每个人的独立博客 # 在列表页会显示完整的专栏标题description: 关于搭建独立博客相关的知识和经验分享，以及 Stellar 的高级用法、版本更新相关的注意事项。order_by: -date # 默认是按发布日期倒序排序icon: https://xxx 文章中的引用 在此专栏文章的 md 文件的 front-matter 部分指定所属的专栏 id （即上一步创建的文件名 Stelalr设置专栏.yml）","tags":["stellar","Hexo","瞎折腾"]},{"title":"Git配置proxy","path":"/2025/09/24/Git配置proxy/","content":"查看git配置 git config --listgit config --global --list 配置proxy 两种proxy方式 走http/https形式：git clone https://github.com/xxxxx 走ssh 形式：git clone git@github.com:xxxxx 走http/https git config --global http.proxy http://127.0.0.1:7890git config --global https.proxy https://127.0.0.1:7890 取消proxy git config --global --unset http.proxygit config --global --unset https.proxy 走ssh 修改 .ssh/config 文件（不存在则新建，Windows 一般在 C:.ssh)文件夹下面新建一个文件 config，注意没有任何后缀名，就叫 config）： # 必须是 github.comHost github.com HostName github.com User git # 走 HTTP 代理 ProxyCommand connect -H 127.0.0.1:7890 %h %p","tags":["Git"]},{"title":"Hello World","path":"/2025/09/23/hello-world/","content":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick Start Create a new post $ hexo new My New Post More info: Writing Run server $ hexo server More info: Server Generate static files $ hexo generate More info: Generating Deploy to remote sites $ hexo deploy More info: Deployment"}]