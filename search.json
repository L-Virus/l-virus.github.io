[{"title":"在Stellar中设置Pandoc","path":"/2025/10/15/在Stellar中设置Pandoc/","content":"前言 在Stellar中默认的公式渲染器功能不太强大，折腾了两小时发现使用Hexo-renderer-pandoc能够很好的解决默认公式渲染器的问题 下载pandoc 首先先到Pandoc的github官方仓库中下载最新的msi（如果你的Windows） 按照教程安装好，记住安装位置 下载hexo-renderer-pandoc 也是一样进入hexo-renderer-pandoc的仓库中 其实进不进都无所谓的，仓库里的README也可以自己参考 1、删除旧配置 npm un hexo-renderer-marked 2、下载hexo-renderer-pandoc npm install hexo-renderer-pandoc _config中的设置 又到大家喜欢的抄配置环节了 # Pandocpandoc: pandocPath: D:/Pandoc/pandoc.exe args: - --mathjax # 启用 MathJax 渲染公式 # - --filter=pandoc-crossref # 这个要求再装一个包来使用交叉引用 - --wrap=preserve # 防止代码换行破坏公式 # - --standalone # 会让整个页面变形 - --metadata - link-citations=true extensions: - +tex_math_dollars # 允许 $...$ 行内公式 - +raw_tex # 保留原始 LaTeX 代码# Markdownmarkdown: render:pandoc","tags":["Hexo","瞎折腾","Stellar"]},{"title":"Attention Is All You Need","path":"/2025/10/15/Attention Is All You Need/","content":"模型 Input与Output 个人认为Input与Output之间画一条指向Output的线，毕竟，Output Embedding的输入只是Input Embedding输入的下一个输入，图中提到的shifted right 类似与滑动窗口往右边移动 Positional Encoding 使用不同频率的 \\(sin\\) 和 \\(cos\\) 函数来作为位置编码，使用两个函数来代表奇数项和偶数项的位置，唯一的超参数为 \\(d_{model}\\) 特征的大小 \\[\\displaylines{PE_{(pos,2i)}=sin(pos/10000^{2i/d_{model}}) \\\\ PE_{pos,2i+1}=cos(pos/10000^{2i/d_{model}})}\\] 这样得到一个与输入embedding大小一样的位置编码矩阵，直接加在embedding中 我们对输入进行分解\\(QK^T=(E+P)(E+P)^T=EE^T+EP^T+PE^T+PP^T\\) \\(EE^T\\)：表示语义间关系 \\(EP^T,PE^T,PP^T\\)：表示引入位置信息的影响 Scaled Dot-Product Attention Step1 首先关于如何计算出 \\(Q、K、V\\) 矩阵 将输入的token进行embedding变换成计算机输入的向量格式 生成三个可学习的参数矩阵 \\(W_q、W_k、W_v\\) 假设有4个输入为 \\(a_i,i \\in [1,2,3,4]\\) ，也就是 \\(X\\) \\(Q\\)矩阵为，\\(W_q \\times a_i,i \\in [1,2,3,4]\\)，也就是 \\(Q=XW_q\\) \\(K\\)矩阵为，\\(W_k \\times a_i,i \\in [1,2,3,4]\\)，也就是 \\(K=XW_k\\) \\(V\\)矩阵为，\\(W_v \\times a_i,i \\in [1,2,3,4]\\)，也就是 \\(V=XW_v\\) Step 2 计算注意力 \\[Attention(Q,K,V)=Softmax(\\frac{QK^{T}}{\\sqrt(d_k)})V\\] 其中 \\(Q\\) 为 \\(R^q\\) 上大小为 \\(n \\times d_k\\) ，\\(K\\) 为 \\(R^k\\) 上大小为 \\(m \\times d_k\\) , \\(d_k\\) 为自定义超参，\\(V\\) 为 \\(R^v\\) 上大小为 \\(m \\times d_v\\) ，最终的大小为 \\(n \\times m\\) Softmax是对最后一个维度进行操作，也就是词汇表维度 Cross-Attention 使用编码器的输出与对应 \\(W_k、W_v\\) 矩阵进行相乘后计算出 \\(K和V\\)，而解码器中的输入作为 \\(Q\\)，进行计算。实际上，\\(Q\\) 为上一个输入之后的隐状态，K和V则表示为新的输入 \\[Q=XW_Q，K=YW_K,V=YW_V\\] Muti-Head Attention \\[\\displaylines{MutiHead(Q,K,V)=Concat(head_1,...head_h)W^o \\\\ Where \\ head_i=Attention(QW_i^q,KW_i^k,VW_i^v)}\\] “In this work we employ h = 8 parallel attention layers, or heads. For each of these we use dk = dv = dmodel/h = 64. Due to the reduced dimension of each head, the total computational cost is similar to that of single-head attention with full dimensionality.” (Vaswani 等, 2023, p. 5) (pdf) 针对head数来缩小d的维度（具体是使用W的大小来设置的），这样能在最后concat的时候保持维度一致 \\(W\\) 的作用：1. 缩小d的维度。2.学习更多不同的参数 并且使用多个头是为了学习不同的模式，所以使用不同的权重信息 FFN \\[FFN=max(0,xW_1+b_1)W_2+b_2\\] 其中，\\(max(0,xW_1+b_1)\\) 为第一次线性变化后使用ReLU进行非线性变化 “The dimensionality of input and output is dmodel = 512, and the inner-layer has dimensionality dff = 2048.” (Vaswani 等, 2023, p. 5) (pdf) \\(W_1\\) 将 \\(x\\) 的512维度扩大到2048，\\(W_2\\) 则又转会512 Tricks 1.Masked Attention “We also modify the self-attention sub-layer in the decoder stack to prevent positions from attending to subsequent positions.” (Vaswani 等, 2023, p. 3) (pdf) mask attention 因为在编码器中是使用自回归模型，模型会看到整个的输入，然而在预测也就是在解码器中，我们不需要看到全部输入，故使用掩码注意力来遮盖后面的部分 改进 1.针对RNN “This inherently sequential nature precludes parallelization within training examples,” (Vaswani 等, 2023, p. 2) (pdf) 这种固有的顺序性质排除了训练示例中的并行化 2.Norm （在三维中，对谁norm，就是把谁完整的切下来） 使用LayerNorm，而不是BathNorm Norm在Transformer中输入的sequence序列长度不一，如果进行batchnorm的话会导致统计量不稳定 优势 1.利用依赖关系进行建模 “allowing modeling of dependencies without regard to their distance in the input or output sequences” (Vaswani 等, 2023, p. 2) (pdf) “In this work we propose the Transformer, a model architecture eschewing recurrence and instead relying entirely on an attention mechanism to draw global dependencies between input and output.” (Vaswani 等, 2023, p. 2) (pdf) 依赖注意力机制来绘制输入和输出之间的全局依赖关系，对依赖关系进行建模，而不考虑它们在输入或输出序列中的距离。 2.进行并行计算 训练 优化器：Adam，\\(\\beta_1=0.9，\\beta_2=0.98，\\beta_3=0.98，\\epsilon=10^{-9}\\) 学习率：\\[l_{rate}=d_{model}^{-0.5}min(step_num^{-0.5},step_num \\times warmup_steps^{-1.5})，\\\\其中warmup_steps=4000\\] 大概意思是训练步骤的学习率线性增加，随后与步骤数的平方反比根成比例地减少学习率。 正则化： dropout（\\(P_{dropout}=0.1\\)）： 在每个子层中，残差连接之前使用dropout 在embedding和positional encoding相加后进行dropout label smoothing：设置 \\(\\epsilon_{ls}=0.1\\) ,虽然对perplexity不利但是有利于提升准确性和BLEU LabelSmoothing设类别数为：\\(K\\) 平滑系数为：\\(\\epsilon \\in [0,1]\\) 平滑后的目标分布为 \\[ y_i^{LS}= \\begin{cases} 1-\\epsilon,\\ 如果i=真实类别, \\\\ \\frac{\\epsilon}{K-1}， elses \\end{cases} \\]","tags":["Transformer","NLP"]},{"title":"使用Pycharm进行远程ssh（以Featurize为例）","path":"/2025/09/24/使用Pycharm进行进程ssh（以Featurize为例）/","content":"介绍应用背景 在使用 Pycharm 专业版的时候进行远程ssh连接服务器（Featurize）的 Python解释器和 Jupyter 远程连接Python 打开Pycharm点击右下角环境配置 点击添加新的解释器 选择基于SSH 选择新建 输入主机、端口以及用户名 以Featurize为例子，打开工作区右边的倒三角形 选择Pycharm连接 进入跟着指南操作即可 # 连接Jupyter 进入工作区打开Terminal 输入jupyter打开jupyter服务 在自己本机上打开终端（win+R），输入cmd，回车进入终端 进行端口映射 在终端输入 ssh -L8888:localhost:8888 featurize@workspace.featurize.cn -p port 上述操作是将Featurize服务器中的8888（Jupyter的服务端口）端口映射到本地，port为你的机子在Featurize中的端口，这些信息都可以在教程中查看。 按下回车，输入密码即可完成端口映射 进入Pycharm，点击右上角切换Jupyter服务器 选择配置Jupyter服务器 进入Featurize找到token 输入服务器地址和找到的令牌","tags":["Python","服务器"]},{"title":"安装PyTorch及Torchvision","path":"/2025/09/24/安装PyTorch及Torchvision/","content":"1.创建虚拟环境 具体查看我之前写的 《在Windows中利用Python的venv和virtualenv创建虚拟环境》 2. 检查显卡版本和CUDA 这种情况是需要电脑上有单独的英伟达的显卡、或者英伟达的显卡和集显这两种情况都是可以的。 3. 下载链接 进入PyTorch官网选择合适自己设备的下载链接 4. 下载 进入cmd 输入上一步获得的下载链接 5. 等待 6. 检测 下载完成后，进入python环境检测torch是否可用 7.下载轮文件 轮文件官网 按CTRL+F 搜索对应版本的文件 使用Motrix进行下载轮文件（只是因为torch太大采用下载器下载，其他可直接使用浏览器下载），版本对应如下 8.下载Torchvision github pytorch和torchvision版本对应官网 我的对应版本为","tags":["Python","Deep Learning"]},{"title":"Stellar设置专栏（topic）","path":"/2025/09/24/Stellar设置专栏/","content":"专栏文件夹的配置 在 blog/source/_data/ 文件夹中创建一个 topic 文件夹，在其中放入各个专栏的描述文件，文件名就是项目的 id： name: Stellar # 在面包屑导航上会显示较短的名字title: Stellar - 每个人的独立博客 # 在列表页会显示完整的专栏标题description: 关于搭建独立博客相关的知识和经验分享，以及 Stellar 的高级用法、版本更新相关的注意事项。order_by: -date # 默认是按发布日期倒序排序icon: https://xxx 文章中的引用 在此专栏文章的 md 文件的 front-matter 部分指定所属的专栏 id （即上一步创建的文件名 Stelalr设置专栏.yml）","tags":["stellar","Hexo","瞎折腾"]},{"title":"Git配置proxy","path":"/2025/09/24/Git配置proxy/","content":"查看git配置 git config --listgit config --global --list 配置proxy 两种proxy方式 走http/https形式：git clone https://github.com/xxxxx 走ssh 形式：git clone git@github.com:xxxxx 走http/https git config --global http.proxy http://127.0.0.1:7890git config --global https.proxy https://127.0.0.1:7890 取消proxy git config --global --unset http.proxygit config --global --unset https.proxy 走ssh 修改 .ssh/config 文件（不存在则新建，Windows 一般在 C:.ssh)文件夹下面新建一个文件 config，注意没有任何后缀名，就叫 config）： # 必须是 github.comHost github.com HostName github.com User git # 走 HTTP 代理 ProxyCommand connect -H 127.0.0.1:7890 %h %p","tags":["Git"]},{"title":"Hello World","path":"/2025/09/23/hello-world/","content":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick Start Create a new post $ hexo new My New Post More info: Writing Run server $ hexo server More info: Server Generate static files $ hexo generate More info: Generating Deploy to remote sites $ hexo deploy More info: Deployment"}]