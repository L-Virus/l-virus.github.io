[{"title":"Word Embedding","path":"/2025/10/26/Word Embedding/","content":"前言 为了探究主流 Word Embedding 技术的变化，故写下这边文章。主要参考车万翔老师的《自然语言处理 基于预训练模型的方法》 Word Embedding 分为两个时期，一个是 Word2Vec 出来之前的远古时期，一个是 Word2Vec 出现之后的现代时期 One-hot Encoding 假设词表 \\(\\mathcal{V}\\)，则大小为 \\(\\mid \\mathcal{V} \\mid\\) ，则词表中第 \\(i\\) 个词 \\(W_i\\) 表示为向量则为， \\[ e_{w_i}=[0,0,\\dots,1,\\dots,0] \\in \\{0,1\\}^{\\mid \\mathcal{V} \\mid} \\] 在该向量中，第 \\(i\\) 个词在第 \\(i\\) 维上被设置成1，其余均为0。 优点 针对变量少，变量相关性不大的数据来说很方便 问题 相似度 尽管两个词在语义上很相近，但是在经过One-hot Encoding 后，两个向量得到的相似度为0 冗余 这种 Encoding 模型会导致参数矩阵变得稀疏 Sparsity 且冗余 缓解办法 引入 WordNet 等语义词典，然后引入它们的共同语义信息作为新的额外特征，从而缓解同义词的独热表示不同的问题。 分布式语义假设 人们在阅读过程中遇到从未见过的词时，通常会根据上下文来推断其含义以及相关属性。 有点类似skip-gram的思想，自监督 分布式语义假设：词的含义可由其上下文的分布进行表示 基于这种假设，可以利用大规模的未标注文本数据，根据每个词的上下文分布对词进行表示。 词语共现频次表 假设 corpoc 中有三句话： - 我 喜欢 自然 语言 处理 。 - 我 爱 深度 学习 。 - 我 喜欢 机器 学习 。 假设以词所在句子中的其他词语作为上下文。构建词语共现频次表 表中的每一项代表一个词\\(wi\\)与另一个词\\(w_j\\)（上下文）在同一个句子中的共现频次，每个词与自身的共现频次设置为0。 我 喜欢 自然 语言 处理 爱 深度 学习 机器 。 我 0 2 1 1 1 1 1 2 1 3 喜欢 2 0 1 1 1 0 0 1 1 2 自然 1 1 0 1 1 0 0 0 0 1 语言 1 1 1 0 1 0 0 0 0 1 处理 1 1 1 1 0 0 0 0 0 1 爱 1 0 0 0 0 1 1 1 0 1 深度 1 0 0 0 0 0 0 1 0 1 学习 2 1 0 0 0 1 1 0 1 1 机器 1 1 0 0 0 0 0 1 0 1 。 3 2 1 1 1 1 1 2 1 0 优势 不再如同 One-hot Encoding 那样每个词向量之间不包含相似度 扩展 上下文的选择有很多种方式，而选择不同的上下文得到的词向量表示性质会有所不同。例如，可以使用词在句子中的一个固定窗口内的词作为其上下文，跟 CBOW 的思想很像。 问题 高频词误导计算结果 共现频次无法反映词之间的高阶关系 仍然存在稀疏性的问题 当矩阵过大时，奇异值分解速度慢 当在原来的语料库基础上新增数据，则需要重新计算奇异值分解 只能表达短的单元，如词、短语，不能表达长的，例如句子、段落 缓解办法 解决高频词误导计算结果 核心思想：如果一个词与很多词共现，则降低其权重；反之，如果一个词只与个别词共现，则提高其权重。 使用点互信息（Pointwise Mutual Information，PMI），对于词 \\(w\\) 和 上下文 \\(c\\)，有 \\[ PMI(w,c)=log_2\\frac{P(w,c)}{P(w)P(c)} \\] 通过这个计算，如果 \\(w\\) 和 \\(c\\) 的共现概率（与频次正相关）较高，但是 \\(w\\) 或者 \\(c\\) 出现的概率也较高（高频词），则最终的\\(PMI\\)值会变小；反之，即便 \\(w\\) 和 \\(c\\) 的共现概率不高，但是 \\(w\\) 或者 \\(c\\) 出现的概率较低（低频词），则最终的\\(PMI\\)值也可能会比较大。从而较好地解决高频词误导计算结果的问题。 由于 \\(log\\) 的分布问题，会导致一些低频率词的大小为负数，使得统计方差变大，因此在实际应用中通常采用PPMI （Positive PMI）\r\\[PPMI(w,c)=max(PMI(w,c),0)\\] 此外还可以使用，信息检索中常用的TF-IDF 共现频次无法反映词之间的高阶关系 以奇异值分解为例，对词语共现矩阵 \\(M\\) 做奇异值分解， \\[ M =U\\Sigma V^{T} \\] 词嵌入表示 为了解决分布式语义假设中的诸多问题，引入这种新的词表示方法 与分布式表示不同的是，词向量中的向量值，是随着目标任务的优化过程自动调整的，也就是说，可以将词向量中的向量值看作模型的参数 词袋模型 词袋模型（Bag of Word BOW）,核心思想就是假设文本中的词语是没有顺序的集合，将文本中的词按照向量进行表示，也就是将整个句子或文档表示为一个词频向量（统计每个词出现次数）。 与One-hot Encoding不同的是，前者是一整个句子或者文档就是一个词频向量，而后者是一个词为一个向量。 与[[#分布式语义假设]]不同的是，前者不考虑词语在句子中的顺序。 例子 词表：[“我”，“爱”，“自然”，“语言”，“处理”] 句子：“我 爱 自然 语言 处理 爱 自然” 对应的BOW向量则为：\\([1, 2, 2, 1, 1]\\) 问题 没考虑到词的顺序信息 无法融入上下文信息，虽然能增加二元词的词向量，但是随着词表的越大，数据稀疏越严重 Word2Vec 预测任务驱动的神经网络模型 输入一般是 One-hot Encoding CBOW 连续词袋模型，基本思想是根据上下文对目标词进行预测 CBOW模型的任务是根据一定窗口大小内的上下文 \\(C_t\\)（若取窗口大小为5，则(\\(C_t=w_{t−2},w_{t−1},w_{t+1},w_{t+2}\\)）对 \\(t\\) 时刻的词 \\(w_t\\) 进行预测，即 \\(P(w_t|C_t)\\) 实际上就是，隐含层执行对词向量层取平均的操作，而没有线性变换以及非线性激活的过程。 Skip-gram 在CBOW的基础上继续做出简化，使用 \\(C_t\\) 中的每个词作为独立的上下文对目标词进行预测。 总计 以上模型可以归纳为对目标词的条件预测任务，CBOW为根据上下文预测当前词，Skip-gram则是根据当前词预测上下文。 负采样方法\r提供了一种新的任务视角：给定当前词与其上下文，最大化两者共现的概率。\r这样问题就被简化为对于（\\(w,c\\)）的二元分类问题（共现或者非共现），规避了大词表上的归一化计算 GloVe 结合词向量以及矩阵分解的思想 基本思想是利用词向量对“词–上下文”共现矩阵进行预测（或者回归），从而实现隐式的矩阵分解 首先根据窗口大小构建窗口词语频次共现矩阵，与词语共现频次表不同的是，这边要根据窗口大小，后者的窗口大小可以认为是整个句子的长度 其中矩阵中每个元素为\\(X_{ij}\\)=词 \\(j\\) 在 词 \\(i\\) 的上下文窗口中出现的次数 将频次转换为概率 \\(P_{ij}=\\frac{X_{ij}}{\\Sigma_kX_{ik}}\\) ，表示词\\(i\\)在上下文看到词\\(j\\)的概率 进行学习词向量，使得 \\(w_i^T \\overset{\\sim}{w_j}+b_i+\\overset{\\sim}{b_j}\\approx log(X_{ij})\\) Why\r使用 \\(log\\) 的含义是对原式子进行缩放，使得，放缩动态范围（减少高频词主导），保持比例关系（共现越多，值越大） ELMo 不管是 Word2Vec 还是 GloVe，都是属于静态词向量，即对于任何一个词，其向量表示是恒定的，不随上下文的变化而变化。 因此为了刻画一个词在不同上下文或不同语境下的不同词义信息，引入动态构建词向量的方法，也称上下文相关的词向量 输入层 为字符级，使用1d CNN，能捕捉词形信息 双向LSTM 同时训练两个模型，正向语言模型预测下一个词 ：\\(P(w_t|w_{1},\\dots,w_{t-1})\\)，反向语言模型预测上一个词：\\(P(W_t|w_{t+1},\\dots,w_T)\\)， 输出层 把每层隐藏层的两个模型的隐状态拼接起来（\\(h_{t,j}=[\\overset{\\leftarrow}{h_{t,j}};\\overset{\\rightarrow}{h_{t,j}}]\\)），再加权组合\\[ELMo_t=\\gamma\\sum{^{L}_{j=0}}s_{j}h_{t,j}\\]其中 \\(\\gamma\\) 为可学习的放缩参数，\\(s_j\\) 为可学习的权重（经过Softmax后），\\(h_{t,j}\\) 为第 \\(j\\) 层的隐藏状 Note\r整个模型的框架就是RNN BERT 根据 BERT 中的描述，主要是利用一个双向的，纯 Transformer 的架构，在 pre-training 中利用 MLM 和 NSP 两个子任务来学习语言表示，再在 Fine-tunning 阶段动态的微调这些语言表示。","tags":["NLP","WordEmbedding"]},{"title":"U-Net：Convolutional Networks for Biomedical  Image Segmentation","path":"/2025/10/26/U-Net：Convolutional Networks for Biomedical  Image Segmentation/","content":"前言 “In this paper, we build upon a more elegant architecture, the so-called “fully convolutional network”” (Ronneberger 等, 2015, p. 2) (pdf) 模型是建立在全卷积网络上 模型 一个U型的结构，左边是下采样进行提取特征，右边是上采样进行恢复特征图，中间是瓶颈层将大量局部特征整合成更高层抽象信息，减少冗余，提供非线性变换，为解码阶段提供新的特征表示，模型在每层下降的过程中会保存下降的结果与对应上升的特征图进行跳跃连接。 输入为 \\(572 \\times 572\\) ，进行下采样每层经过 \\(3 \\times 3\\) 无填充的卷积、ReLU和一个大小为 \\(2 \\times 2\\)，步长为2 的maxpooling，同时增加一倍通道数 因为在上采样过程中图像大小与在下采样中不同，使用需要裁剪特征图来跳跃连接 \\(1 \\times 572 \\times 572\\) 经过两次卷积和两次ReLU后，大小为 \\(64 \\times 568 \\times 568\\) (572-3+1-3+1) ；\r经过maxpooling 后减少特征图大小，大小为 \\(64 \\times 284 \\times 284\\) ((568-2+2)/2)；\r直到最底层大小为 \\(512 \\times 32 \\times 32\\)\r达到瓶颈层后经历两次卷积块后大小为 \\(1024 \\times 28 \\times 28\\)\r在上采样阶段使用反卷积和下降的特征图做拼接，并且通道数量减半到大小为 \\(64 \\times 388 \\times 388\\)\r最后进行 \\(1 \\times 1\\) 卷积，将通道数降到图像中类别数\r在后续的U-net优化中，应该使上下采样的大小相同 模型优化 1.重叠切片（无缝平铺）+镜像填充 因为卷积网络在经过下采样和上采样后，图片中的物体边缘精度会降低，因为在U-net中使用一种名为重叠切片+镜像填充的方法解决 2.弹性形变 使用弹性形变来进行充分的数据增强，因为在医学图像中，组织的形变是最常见的变化，Dosovitskiy [Dosovitskiy, A., Springenberg, J.T., Riedmiller, M., Brox, T.: Discriminative unsupervised feature learning with convolutional neural networks. In: NIPS (2014)] 在无监督特征学习的范围内已经证明了数据增强对学习不变性的价值。 3.加权损失 为了解决细胞分割中的一个挑战：分离同一类的接触物体； U-net使用加权损失的方法：接触细胞之间的分离背景标签在损失函数中获得较大的权重。 这样能更好的分离出两个接触细胞之间的背景信息 在后续的优化中，我们更通常使用Dice loss 或 Focal loss，Boundary loss / Hausdorff loss 4.选择合适的池化窗口大小 为了保证 tile 能被网络多层池化后尺寸仍然对齐，需要选择合适的 tile 大小（通常是 \\(2^N\\) 的倍数 + U-Net 卷积裁剪调整）。 数据 ISBI 2015 训练 优化器：SGD batch_size：1 momentum：0.99 loss：softmax、cross-entropy init_weight：使用标准差为 \\(\\sqrt{2/N}\\) 的高斯分布，其中N为每层输入的像素点数 数据增强 “We generate smooth deformations using random displacement vectors on a coarse 3 by 3 grid. The displacements are sampled from a Gaussian distribution with 10 pixels standard deviation. Per-pixel displacements are then computed using bicubic interpolation. Drop-out layers at the end of the contracting path perform further implicit data augmentation.” (Ronneberger 等, 2015, p. 6) (pdf) 在粗糙的 3 x 3 网格上使用随机位移矢量生成平滑变形。位移是从具有 10 像素标准差的高斯分布中采样的。然后使用双三次插值计算每像素位移。收缩路径末端的辍学层执行进一步的隐式数据扩充。 以及正常的旋转和变色","tags":["U-net，CV"]},{"title":"Conda的使用","path":"/2025/10/26/Conda的使用/","content":"一、Conda的管理 1.1 检查Conda版本 conda --version 1.2 列出虚拟环境 conda env list 二、管理虚拟环境 2.1 创建环境 conda create -n env-name python==3.x 2.2 激活环境 基础环境为base windows：conda activate env-name linux： source activate env-name 2.3 退出环境 windows：conda deactivate env-name linux： source deactivate env-name 2.4 删除环境 conda env remove -n env-name","tags":["Conda"]},{"title":"在Stellar中设置Pandoc","path":"/2025/10/15/在Stellar中设置Pandoc/","content":"前言 在Stellar中默认的公式渲染器功能不太强大，折腾了两小时发现使用Hexo-renderer-pandoc能够很好的解决默认公式渲染器的问题 下载pandoc 首先先到Pandoc的github官方仓库中下载最新的msi（如果你的Windows） 按照教程安装好，记住安装位置 下载hexo-renderer-pandoc 也是一样进入hexo-renderer-pandoc的仓库中 其实进不进都无所谓的，仓库里的README也可以自己参考 1、删除旧配置 npm un hexo-renderer-marked 2、下载hexo-renderer-pandoc npm install hexo-renderer-pandoc _config中的设置 又到大家喜欢的抄配置环节了 # Pandocpandoc: pandocPath: D:/Pandoc/pandoc.exe args: - --mathjax # 启用 MathJax 渲染公式 # - --filter=pandoc-crossref # 这个要求再装一个包来使用交叉引用 - --wrap=preserve # 防止代码换行破坏公式 # - --standalone # 会让整个页面变形 - --metadata - link-citations=true extensions: - +tex_math_dollars # 允许 $...$ 行内公式 - +raw_tex # 保留原始 LaTeX 代码# Markdownmarkdown: render:pandoc","tags":["Hexo","瞎折腾","Stellar"]},{"title":"Attention Is All You Need","path":"/2025/10/15/Attention Is All You Need/","content":"模型 Input与Output 个人认为Input与Output之间画一条指向Output的线，毕竟，Output Embedding的输入只是Input Embedding输出的下一个输入，图中提到的shifted right 类似与滑动窗口往右边移动 Positional Encoding 使用不同频率的 \\(sin\\) 和 \\(cos\\) 函数来作为位置编码，使用两个函数来代表奇数项和偶数项的位置，唯一的超参数为 \\(d_{model}\\) 特征的大小 \\[\\displaylines{PE_{(pos,2i)}=sin(pos/10000^{2i/d_{model}}) \\\\ PE_{pos,2i+1}=cos(pos/10000^{2i/d_{model}})}\\] 这样得到一个与输入embedding大小一样的位置编码矩阵，直接加在embedding中 我们对输入进行分解\\(QK^T=(E+P)(E+P)^T=EE^T+EP^T+PE^T+PP^T\\) \\(EE^T\\)：表示语义间关系 \\(EP^T,PE^T,PP^T\\)：表示引入位置信息的影响 Scaled Dot-Product Attention Step1 首先关于如何计算出 \\(Q、K、V\\) 矩阵 将输入的token进行embedding变换成计算机输入的向量格式 生成三个可学习的参数矩阵 \\(W_q、W_k、W_v\\) 假设有4个输入为 \\(a_i,i \\in [1,2,3,4]\\) ，也就是 \\(X\\) \\(Q\\)矩阵为，\\(W_q \\times a_i,i \\in [1,2,3,4]\\)，也就是 \\(Q=XW_q\\) \\(K\\)矩阵为，\\(W_k \\times a_i,i \\in [1,2,3,4]\\)，也就是 \\(K=XW_k\\) \\(V\\)矩阵为，\\(W_v \\times a_i,i \\in [1,2,3,4]\\)，也就是 \\(V=XW_v\\) Step 2 计算注意力 \\[Attention(Q,K,V)=Softmax(\\frac{QK^{T}}{\\sqrt(d_k)})V\\] 其中 \\(Q\\) 为 \\(R^q\\) 上大小为 \\(n \\times d_k\\) ，\\(K\\) 为 \\(R^k\\) 上大小为 \\(m \\times d_k\\) , \\(d_k\\) 为自定义超参，\\(V\\) 为 \\(R^v\\) 上大小为 \\(m \\times d_v\\) ，最终的大小为 \\(n \\times m\\) Softmax是对最后一个维度进行操作，也就是词汇表维度 Cross-Attention 使用编码器的输出与对应 \\(W_k、W_v\\) 矩阵进行相乘后计算出 \\(K和V\\)，而解码器中的输入作为 \\(Q\\)，进行计算。实际上，\\(Q\\) 为上一个输入之后的隐状态，K和V则表示为新的输入 \\[Q=XW_Q，K=YW_K,V=YW_V\\] Muti-Head Attention \\[\\displaylines{MutiHead(Q,K,V)=Concat(head_1,...head_h)W^o \\\\ Where \\ head_i=Attention(QW_i^q,KW_i^k,VW_i^v)}\\] “In this work we employ h = 8 parallel attention layers, or heads. For each of these we use dk = dv = dmodel/h = 64. Due to the reduced dimension of each head, the total computational cost is similar to that of single-head attention with full dimensionality.” (Vaswani 等, 2023, p. 5) (pdf) 针对head数来缩小d的维度（具体是使用W的大小来设置的），这样能在最后concat的时候保持维度一致 \\(W\\) 的作用：1. 缩小d的维度。2.学习更多不同的参数 并且使用多个头是为了学习不同的模式，所以使用不同的权重信息 FFN \\[FFN=max(0,xW_1+b_1)W_2+b_2\\] 其中，\\(max(0,xW_1+b_1)\\) 为第一次线性变化后使用ReLU进行非线性变化 “The dimensionality of input and output is dmodel = 512, and the inner-layer has dimensionality dff = 2048.” (Vaswani 等, 2023, p. 5) (pdf) \\(W_1\\) 将 \\(x\\) 的512维度扩大到2048，\\(W_2\\) 则又转会512 Tricks 1.Masked Attention “We also modify the self-attention sub-layer in the decoder stack to prevent positions from attending to subsequent positions.” (Vaswani 等, 2023, p. 3) (pdf) mask attention 因为在编码器中是使用自回归模型，模型会看到整个的输入，然而在预测也就是在解码器中，我们不需要看到全部输入，故使用掩码注意力来遮盖后面的部分 改进 1.针对RNN “This inherently sequential nature precludes parallelization within training examples,” (Vaswani 等, 2023, p. 2) (pdf) 这种固有的顺序性质排除了训练示例中的并行化 2.Norm （在三维中，对谁norm，就是把谁完整的切下来） 使用LayerNorm，而不是BathNorm Norm在Transformer中输入的sequence序列长度不一，如果进行batchnorm的话会导致统计量不稳定 优势 1.利用依赖关系进行建模 “allowing modeling of dependencies without regard to their distance in the input or output sequences” (Vaswani 等, 2023, p. 2) (pdf) “In this work we propose the Transformer, a model architecture eschewing recurrence and instead relying entirely on an attention mechanism to draw global dependencies between input and output.” (Vaswani 等, 2023, p. 2) (pdf) 依赖注意力机制来绘制输入和输出之间的全局依赖关系，对依赖关系进行建模，而不考虑它们在输入或输出序列中的距离。 2.进行并行计算 训练 优化器：Adam，\\(\\beta_1=0.9，\\beta_2=0.98，\\beta_3=0.98，\\epsilon=10^{-9}\\) 学习率：\\[l_{rate}=d_{model}^{-0.5}min(step_num^{-0.5},step_num \\times warmup_steps^{-1.5})，\\\\其中warmup_steps=4000\\] 大概意思是训练步骤的学习率线性增加，随后与步骤数的平方反比根成比例地减少学习率。 正则化： dropout（\\(P_{dropout}=0.1\\)）： 在每个子层中，残差连接之前使用dropout 在embedding和positional encoding相加后进行dropout label smoothing：设置 \\(\\epsilon_{ls}=0.1\\) ,虽然对perplexity不利但是有利于提升准确性和BLEU LabelSmoothing设类别数为：\\(K\\) 平滑系数为：\\(\\epsilon \\in [0,1]\\) 平滑后的目标分布为 \\[ y_i^{LS}= \\begin{cases} 1-\\epsilon,\\ 如果i=真实类别, \\\\ \\frac{\\epsilon}{K-1}， elses \\end{cases} \\]","tags":["Transformer","NLP"]},{"title":"使用Pycharm进行远程ssh（以Featurize为例）","path":"/2025/09/24/使用Pycharm进行进程ssh（以Featurize为例）/","content":"介绍应用背景 在使用 Pycharm 专业版的时候进行远程ssh连接服务器（Featurize）的 Python解释器和 Jupyter 远程连接Python 打开Pycharm点击右下角环境配置 点击添加新的解释器 选择基于SSH 选择新建 输入主机、端口以及用户名 以Featurize为例子，打开工作区右边的倒三角形 选择Pycharm连接 进入跟着指南操作即可 # 连接Jupyter 进入工作区打开Terminal 输入jupyter打开jupyter服务 在自己本机上打开终端（win+R），输入cmd，回车进入终端 进行端口映射 在终端输入 ssh -L8888:localhost:8888 featurize@workspace.featurize.cn -p port 上述操作是将Featurize服务器中的8888（Jupyter的服务端口）端口映射到本地，port为你的机子在Featurize中的端口，这些信息都可以在教程中查看。 按下回车，输入密码即可完成端口映射 进入Pycharm，点击右上角切换Jupyter服务器 选择配置Jupyter服务器 进入Featurize找到token 输入服务器地址和找到的令牌","tags":["Python","服务器"]},{"title":"安装PyTorch及Torchvision","path":"/2025/09/24/安装PyTorch及Torchvision/","content":"1.创建虚拟环境 具体查看我之前写的 《在Windows中利用Python的venv和virtualenv创建虚拟环境》 2. 检查显卡版本和CUDA 这种情况是需要电脑上有单独的英伟达的显卡、或者英伟达的显卡和集显这两种情况都是可以的。 3. 下载链接 进入PyTorch官网选择合适自己设备的下载链接 4. 下载 进入cmd 输入上一步获得的下载链接 5. 等待 6. 检测 下载完成后，进入python环境检测torch是否可用 7.下载轮文件 轮文件官网 按CTRL+F 搜索对应版本的文件 使用Motrix进行下载轮文件（只是因为torch太大采用下载器下载，其他可直接使用浏览器下载），版本对应如下 8.下载Torchvision github pytorch和torchvision版本对应官网 我的对应版本为","tags":["Python","Deep Learning"]},{"title":"Stellar设置专栏（topic）","path":"/2025/09/24/Stellar设置专栏/","content":"专栏文件夹的配置 在 blog/source/_data/ 文件夹中创建一个 topic 文件夹，在其中放入各个专栏的描述文件，文件名就是项目的 id： name: Stellar # 在面包屑导航上会显示较短的名字title: Stellar - 每个人的独立博客 # 在列表页会显示完整的专栏标题description: 关于搭建独立博客相关的知识和经验分享，以及 Stellar 的高级用法、版本更新相关的注意事项。order_by: -date # 默认是按发布日期倒序排序icon: https://xxx 文章中的引用 在此专栏文章的 md 文件的 front-matter 部分指定所属的专栏 id （即上一步创建的文件名 Stelalr设置专栏.yml）","tags":["stellar","Hexo","瞎折腾"]},{"title":"Git配置proxy","path":"/2025/09/24/Git配置proxy/","content":"查看git配置 git config --listgit config --global --list 配置proxy 两种proxy方式 走http/https形式：git clone https://github.com/xxxxx 走ssh 形式：git clone git@github.com:xxxxx 走http/https git config --global http.proxy http://127.0.0.1:7890git config --global https.proxy https://127.0.0.1:7890 取消proxy git config --global --unset http.proxygit config --global --unset https.proxy 走ssh 修改 .ssh/config 文件（不存在则新建，Windows 一般在 C:.ssh)文件夹下面新建一个文件 config，注意没有任何后缀名，就叫 config）： # 必须是 github.comHost github.com HostName github.com User git # 走 HTTP 代理 ProxyCommand connect -H 127.0.0.1:7890 %h %p","tags":["Git"]},{"title":"Hello World","path":"/2025/09/23/hello-world/","content":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick Start Create a new post $ hexo new My New Post More info: Writing Run server $ hexo server More info: Server Generate static files $ hexo generate More info: Generating Deploy to remote sites $ hexo deploy More info: Deployment"}]